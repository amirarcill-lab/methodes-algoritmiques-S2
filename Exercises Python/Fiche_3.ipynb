{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ab16eea"
      },
      "source": [
        "## Exercice 1: Création du dictionnaire personne\n",
        "\n",
        "### Subtask:\n",
        "Créer un dictionnaire nommé 'personne' avec les clés 'prenom', 'nom', 'age' et 'ville', en attribuant des valeurs de votre choix.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26559cbf",
        "outputId": "b7063db6-fb7b-4ed7-a920-97a881235c9c"
      },
      "source": [
        "personne = {\n",
        "    'prenom': 'Amira',\n",
        "    'nom': 'Rpdriguez',\n",
        "    'age': 21,\n",
        "    'ville': 'Paris'\n",
        "}\n",
        "\n",
        "print(personne)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prenom': 'Alice', 'nom': 'Dupont', 'age': 30, 'ville': 'Paris'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "867a92cc"
      },
      "source": [
        "## Exercice 2: Accès aux valeurs du dictionnaire\n",
        "\n",
        "### Subtask:\n",
        "Afficher le prénom et l'âge de la personne à partir du dictionnaire 'personne'.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6a03de2",
        "outputId": "af7a0713-6334-4c66-c9f0-13f7551330a6"
      },
      "source": [
        "prenom = personne['prenom']\n",
        "age = personne['age']\n",
        "\n",
        "print(f\"Prénom: {prenom}\")\n",
        "print(f\"Âge: {age}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prénom: Alice\n",
            "Âge: 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd66f97e"
      },
      "source": [
        "## Exercice 3: Modification d'une valeur et affichage\n",
        "\n",
        "### Subtask:\n",
        "Modifier l'âge de la personne dans le dictionnaire 'personne', puis afficher le dictionnaire mis à jour.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd7214c6",
        "outputId": "e69221aa-37d6-4ae6-c58e-bb07243e7c33"
      },
      "source": [
        "personne['age'] = 31\n",
        "\n",
        "print(personne)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prenom': 'Alice', 'nom': 'Dupont', 'age': 31, 'ville': 'Paris'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "414f395b"
      },
      "source": [
        "## Exercice 4: Ajout d'une nouvelle clé\n",
        "\n",
        "### Subtask:\n",
        "Ajouter une nouvelle clé 'profession' au dictionnaire 'personne' avec une valeur de votre choix.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e154543",
        "outputId": "48f79ef7-54a6-4536-8b08-e9c7d45b2c08"
      },
      "source": [
        "personne['profession'] = 'Étudiante'\n",
        "\n",
        "print(personne)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prenom': 'Alice', 'nom': 'Dupont', 'age': 31, 'ville': 'Paris', 'profession': 'Étudiante'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "553a748d"
      },
      "source": [
        "## Exercice 5: Parcourir les clés du dictionnaire\n",
        "\n",
        "### Subtask:\n",
        "Afficher la liste des clés du dictionnaire 'personne'.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b978be90",
        "outputId": "6d4bdfa1-3db5-470e-f3c0-b8ca7714b6a9"
      },
      "source": [
        "keys_personne = personne.keys()\n",
        "print(keys_personne)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['prenom', 'nom', 'age', 'ville', 'profession'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25f9f7bd"
      },
      "source": [
        "## Exercice 6: Analyse de texte et comptage de mots\n",
        "Créer une variable 'texte' avec un texte de 1000 à 2000 caractères. Compter la fréquence de chaque mot, afficher les 10 mots les plus utilisés, et discuter des bonnes pratiques pour l'analyse de texte. Une solution pour cet exercice se trouve dans 'sol_3_ex_dictionnaires_ex6.ipynb'.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae0c753f"
      },
      "source": [
        "texte = \"An incel (a portmanteau of involuntary celibate) is a member of an online subculture of mostly male and heterosexual people who define themselves as unable to find a romantic or sexual partner despite desiring one. They often blame, objectify, and denigrate women and girls as a result. The term inspired a subculture that rose to prominence during the 2010s, after being influenced by and associated with misogynist terrorists such as Elliot Rodger and Alek Minassian. The incel subculture's online discourse has been characterized by resentment, hostile sexism, anti-feminism, sexual objectification and dehumanization of women, misogyny, misanthropy, self-pity and self-loathing, racism, a sense of entitlement to sex, nihilism, rape culture, and the endorsement of sexual and non-sexual violence against women and the sexually active. Incels tend to blame women and feminism for their inability to find a partner; their romantic failures are often attributed to biological determinism, where women's preference for mating with high-status males nicknamed Chads is seen as innate and unchangeable. Incel communities have been criticized by scholars, government officials, and others for their misogyny, endorsement and encouragement of violence, and extremism. Over time the subculture has become associated with extremism and terrorism, and since 2014 there have been multiple mass killings, mostly in North America, perpetrated by self-identified incels, as well as other instances of violence or attempted violence. The Southern Poverty Law Center SPLC describes incels as part of the online male supremacist ecosystem that is included in their list of hate groups. The Global Internet Forum to Counter Terrorism GIFCT states that the incel community shares a misogynistic ideology of women as being genetically inferior to men, driven by their sexual desire to reproduce with genetically superior males, thereby excluding unattractive men such as themselves which exhibits all of the hallmarks of an extremist ideology; GIFCT states that incel beliefs combine a wish for a mythical past where all men were entitled to sex from subordinated women, a sense of predestined personal failure, and nihilism, making it a dangerous ideology. Estimates of the overall size of the subculture vary greatly, ranging from thousands to hundreds of thousands of individuals.\""
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfcff0e8",
        "outputId": "8c291e3c-a5ac-4003-de9f-0718cda85f79"
      },
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "# 1. Convert text to lowercase\n",
        "texte_lower = texte.lower()\n",
        "\n",
        "# 2. Remove punctuation\n",
        "texte_no_punct = texte_lower.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# 3. Split into words\n",
        "mots = texte_no_punct.split()\n",
        "\n",
        "# 4. Count word frequencies using Counter\n",
        "frequence_mots_counter = Counter(mots)\n",
        "\n",
        "# 5. Get the 10 most common words\n",
        "top_10_mots = frequence_mots_counter.most_common(10)\n",
        "\n",
        "print(\"Top 10 most frequent words:\")\n",
        "for mot, count in top_10_mots:\n",
        "    print(f\"  {mot}: {count}\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most frequent words:\n",
            "  of: 18\n",
            "  and: 18\n",
            "  the: 13\n",
            "  a: 12\n",
            "  to: 11\n",
            "  as: 9\n",
            "  women: 6\n",
            "  incel: 5\n",
            "  by: 5\n",
            "  their: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed347a5"
      },
      "source": [
        "### Bonnes pratiques pour l'analyse de texte\n",
        "\n",
        "L'analyse de texte est un processus qui permet d'extraire des informations significatives et structurées à partir de données textuelles. Pour garantir l'exactitude et la pertinence des résultats, il est essentiel d'appliquer certaines bonnes pratiques de prétraitement.\n",
        "\n",
        "1.  **Normalisation du texte (Text Normalization)**:\n",
        "    *   **Mise en minuscules (Lowercasing)**: Convertir tout le texte en minuscules est une étape cruciale pour traiter des mots comme 'Pomme' et 'pomme' comme le même terme. Cela réduit la complexité et la taille du vocabulaire.\n",
        "    *   **Suppression de la ponctuation (Punctuation Removal)**: La ponctuation (points, virgules, points d'interrogation, etc.) n'apporte généralement pas de valeur sémantique pour l'analyse de fréquence des mots et peut créer des tokens distincts inutiles (ex: 'mot.' vs 'mot'). Sa suppression simplifie le texte et améliore la précision du comptage des mots, comme démontré dans les étapes précédentes de cet exercice.\n",
        "\n",
        "2.  **Suppression des mots vides (Stop Word Removal)**:\n",
        "    *   Les mots vides (ou \"stop words\") sont des mots courants (comme 'le', 'la', 'et', 'un', 'de', 'à' en français ou 'the', 'a', 'is' en anglais) qui apparaissent très fréquemment dans la langue mais qui n'apportent généralement pas beaucoup d'informations sémantiques ou discriminantes pour l'analyse. Les supprimer permet de réduire la taille du jeu de données et de se concentrer sur les mots plus significatifs. Des bibliothèques comme `nltk` en Python fournissent des listes prédéfinies de mots vides pour diverses langues.\n",
        "\n",
        "3.  **Radicalisation ou Lemmatisation (Stemming or Lemmatization)**:\n",
        "    *   **Radicalisation (Stemming)**: C'est le processus de réduction des mots à leur forme racine ou à leur \"radical\" (stem). Par exemple, \"courir\", \"couraient\", \"court\" pourraient tous être réduits à \"cour\". Le radical n'est pas toujours un mot grammaticalement correct.\n",
        "    *   **Lemmatisation (Lemmatization)**: Similaire à la radicalisation, mais plus sophistiquée. Elle réduit les mots à leur forme de base (lemme) grammaticalement correcte en utilisant un vocabulaire et une analyse morphologique du mot. Par exemple, \"meilleur\" serait lemmatisé en \"bon\", et \"cours\" (nom) et \"cours\" (verbe) seraient distingués. La lemmatisation est généralement plus précise que la radicalisation mais aussi plus coûteuse en termes de calcul.\n",
        "\n",
        "Ces techniques de prétraitement sont fondamentales pour préparer les données textuelles à des analyses plus avancées comme la modélisation de sujets, la classification de texte ou l'analyse de sentiments, en améliorant la qualité des caractéristiques extraites des textes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b99b61e"
      },
      "source": [
        "## Final Task\n",
        "Fournir un résumé des exercices réalisés et des concepts de dictionnaires et d'analyse de texte abordés.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce005409"
      },
      "source": [
        "## Summary:\n",
        "*   **Python Dictionary Fundamentals**:\n",
        "    *   A dictionary named `personne` was successfully created and manipulated, demonstrating creation, accessing values, modifying existing values, and adding new key-value pairs.\n",
        "    *   Initial values for `personne` included 'prenom': 'Alice', 'nom': 'Dupont', 'age': 30, and 'ville': 'Paris'.\n",
        "    *   The 'age' was successfully updated from 30 to 31.\n",
        "    *   A new key 'profession' with the value 'Ingénieur logiciel' was successfully added.\n",
        "    *   All keys of the `personne` dictionary ('prenom', 'nom', 'age', 'ville', 'profession') were successfully retrieved and displayed.\n",
        "*   **Text Analysis - Word Frequency Counting**:\n",
        "    *   A sample text exceeding 1000 characters was successfully processed.\n",
        "    *   The text underwent normalization, including conversion to lowercase and removal of punctuation, to ensure accurate word counting.\n",
        "    *   Word tokenization was performed, splitting the cleaned text into individual words.\n",
        "    *   The frequency of each word was calculated, identifying 146 unique words in the sample text.\n",
        "    *   The top 10 most frequent words were successfully identified and displayed, including terms like 'vel', 'id', 'libero', 'ipsum', and 'bibendum'.\n",
        "*   **Text Analysis Best Practices**: Key best practices for text analysis were discussed, covering:\n",
        "    *   **Normalization**: Emphasizing lowercasing and punctuation removal to standardize text.\n",
        "    *   **Stop Word Removal**: Explaining the importance of removing common, semantically insignificant words.\n",
        "    *   **Stemming and Lemmatization**: Differentiating between reducing words to their root or base forms for consistent analysis.\n"
      ]
    }
  ]
}